<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>VASBSD 2021 - Vision Applications & Solutions to Biased or Scarce Data</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/wacv21.png" rel="icon">
  <link href="img/wacv21.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- =======================================================
    Theme Name: TheEvent
    Theme URL: https://bootstrapmade.com/theevent-conference-event-bootstrap-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body>

  <!--==========================
    Header
  ============================-->
  <header id="header" class="header-fixed">
    <div class="container">

      <div id="logo" class="pull-left">
        <!-- Uncomment below if you prefer to use a text logo -->
        <!-- <h1><a href="#main">C<span>o</span>nf</a></h1>-->
        <!-- <a href="index.html#intro" class="scrollto"><img src="img/logo.png" alt="" title=""></a> -->
        <a href="index.html#intro" class="scrollto"><p style="font-size:30px;color:white">VASBSD 2021</p></a>
      </div>

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li class="menu-active"><a href="index.html#intro">Home</a></li>
          <li><a href="index.html#about">About</a></li>
          <li><a href="index.html#speakers">Speakers</a></li>
          <li><a href="index.html#schedule">Schedule</a></li>
          <li><a href="index.html#venue">Venue</a></li>
          <!-- <li><a href="#hotels">Hotels</a></li> -->
          <!-- <li><a href="#gallery">Gallery</a></li> -->
          <!-- <li><a href="#supporters">Sponsors</a></li> -->
          <!-- <li><a href="#contact">Contact</a></li> -->
          <li><a href="index.html#organizers">Contact</a></li>
          <li><a href="../index.html#VASBSD_series">VASBSD Series</a></li>
          <!-- <li class="buy-tickets"><a href="#buy-tickets">Buy Tickets</a></li> -->
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->

  <main id="main" class="main-page">

    <!--==========================
      Speaker Details Section
    ============================-->
    <section id="speakers-details" class="wow fadeIn">
      <div class="container">
        <div class="section-header">
          <h2>Speaker Details</h2>
          <!-- <p>TBD</p> -->
        </div>

        
        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Andrew_Bagdanov.jpg" alt="Andrew D. Bagdanov" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Andrew_Bagdanov"><a href="http://www.micc.unifi.it/bagdanov/">Andrew D. Bagdanov</a></h2>
              <p>University of Florence</p>
              <div class="social"> 
                <!--
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
                <a href="https://www.linkedin.com/in/andy-bagdanov-1228146/"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Andrew D. Bagdanov received a PhD in Computer Science in 2004 from the University of Amsterdam. He held postdoctoral positions at the University of Florence and the Universidad Autonoma de Barcelona, and a senior development position at the FAO of the United Nations. He is currently an Associate Professor of Information Engineering at the University of Florence, Italy. His research spans a broad gamut of image processing, computer vision, and machine learning.</p>
              <p>Keynote Title:<br>Self-supervised Learning: Getting More for Less out of Your CNNs</p>
			  <p>Keynote Abstract:<br>The advent of viable, deep neural network architectures for visual recognition continues to radically reshape the landscape of the state-of-the-art in computer vision. These developments have brought amazing new possibilities and formidable new challenges to the table. In this talk we will see how self-supervised learning approaches can be designed and implemented in order to mitigate the data-hungry nature of Convolutional Neural Networks and dramatically reduce the burden of supervision. We will see how to design proxy-tasks for supervision and how they can be exploited to learn semantically meaningful visual representations for recognition problems. Self-supervised learning, with carefully crafted proxy tasks, can address critical issues in training and deployment of state-of-the-art CNNs and enable one to get more from CNNs for less -- that is, to obtain state-of-the-art results on multiple visual recognition tasks with fewer labeled training examples.</p> 
			  <a href="https://drive.google.com/file/d/1VRLFrLUBKYsae5PmjPffm8JchJusorqP/view?usp=sharing">[Talk Video]</a>
            </div>
          </div>  
        </div>


        <p></p>

        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Ehsan_Elhamifar.jpg" alt="Ehsan Elhamifar" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Ehsan_Elhamifar"><a href="https://khoury.neu.edu/home/eelhami/">Ehsan Elhamifar</a></h2>
              <p>Northeastern University</p>
              <div class="social">
			    <a href="https://www.linkedin.com/in/ehsan-elhamifar-b43ab396/"><i class="fa fa-linkedin"></i></a>
                <!--
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
              </div>
              <p>Bio:<br>Ehsan Elhamifar is currently an Assistant Professor in the Khoury College of Computer Sciences and is the director of the Mathematical Data Science (MCADS) Lab at Northeastern University. Dr. Elhamifar is a recipient of the DARPA Young Faculty Award and the NSF CISE Career Research Initiation Initiative Award. Previously, he was a postdoctoral scholar in the Electrical Engineering and Computer Science department at UC Berkeley. He obtained his PhD from the Electrical and Computer Engineering department at the Johns Hopkins University. Dr. Elhamifar's research areas are machine learning, computer vision and optimization. He develops scalable, robust and interpretable methods for learning from complex and massive high-dimensional data and works on applications of these tools to structured visual data summarization, learning instructions from videos and large-scale recognition with small/no labeled data.</p>
			  <p>Keynote Title:<br>Recognizing More Labels with No Samples: Fine-Grained and Multi-Label Zero-Shot Learning</p>
	          <p>Keynote Abstract:<br>The success of deep neural networks (DNNs) on a variety of tasks, such as recognition, detection, semantic segmentation and captioning, relies on the availability of large amounts of annotated data. However, to successfully support real-world applications, DNNs must learn tens of thousands of labels, handle a priori unseen labels and localize them in images. In this talk, I discuss learning high performance and robust DNNs for recognition of a large number of labels, many of which have small or no annotated images. I will focus on two challenging scenarios: i) fine-grained zero-shot recognition and localization where unseen and seen labels are visually similar, requiring extremely expensive annotations by experts, ii) multi-label zero-shot recognition and localization where all existing labels in an image must be found, as opposed to the conventional problem of finding the dominant class. I discuss methods based on an ensemble of label-agnostic and attribute-based attention models that capture both common and discriminative information about labels, hence not only perform well for prediction of seen labels, but also transfer attention to unseen labels. I show that these new methods significantly improve the state of the art on large-scale image datasets and standard benchmarks.</p>
			  <a href="https://drive.google.com/file/d/1OkuYvyIvpU7FRp1pF4ElL3EbEE4BSW16/view?usp=sharing">[Talk Video]</a>
            </div>
          </div>  
        </div>

        <p></p>

        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Mehrtash_Harandi.jpg" alt="Mehrtash Harandi" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Mehrtash_Harandi"><a href="https://sites.google.com/site/mehrtashharandi/">Mehrtash Harandi</a></h2>
              <p>Monash University</p>
              <div class="social">
			    <a href="https://www.facebook.com/mharandi"><i class="fa fa-facebook"></i></a>
                <a href="https://www.linkedin.com/in/mehrtash-harandi-b99358155/"><i class="fa fa-linkedin"></i></a>
                <!--
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
              </div>
              <p>Bio:<br>Dr. Mehrtash Harandi is a senior lecturer in the department of Electrical and Computer Systems Eng. (ECSE) at Monash University. He is also a contributing research scientist at the Machine Learning Research Group (MLRG) at Data61-CSIRO and an associated investigator at Australian Center for Robotic Vision (ACRV).</p>
			  <p>Before joining Monash University, he spent 5 years at Canberra Research Laboratory-NICTA, working with Prof. Richard Hartley and Prof. Fatih Porikli. Prior to that, he worked at Queensland Research Laboratory-NICTA with Prof. Brian Lovell.</p>
			  <p>He is interested in various aspects of learning, especially with a flavor of visual data. He is an associate editor of the IET-CV and Journal of Imaging, and he regularly reviews papers for top conferences and journals in ML/CV including CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, IEEE TPAMI, IEEE TNNLS, IEEE TIP. </p>
              <p>Keynote Title:<br>Poincaré Kernels for Hyperbolic Representations</p>
              <p>Keynote Abstract:<br>Embedding data in hyperbolic spaces has proven beneficial for many advanced machine learning applications such as image classification and word embeddings. However, working in hyperbolic spaces is not without difficulties as a result of its curved geometry (eg, computing the mean of a set of points requires an iterative algorithm).
In Euclidean spaces, one can resort to kernel machines that not only enjoy rich theoretical properties but also can lead to a superior representational power (eg, infinite-width neural networks). In this talk, we introduce positive definite kernel functions for hyperbolic spaces. This brings in two major advantages, 1. kernelization will pave the way to seamlessly benefit from kernel machines in conjunction with hyperbolic embeddings, and 2. the rich structure of the Hilbert spaces associated with kernel machines enables us to simplify various operations involving hyperbolic data. That said, identifying valid kernel functions is not straightforward and is indeed considered an open-problem in the learning community. Our work here addresses this gap and develops several valid positive definite Poincaré kernels for hyperbolic representations, including the universal ones (eg, RBF). We study the effectiveness of the proposed hyperbolic kernels on a variety of challenging tasks including few-shot learning and zero-shot learning. </p>
            </div>
          </div>
        </div>

        <p></p>

        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Gim_Hee_Lee.jpg" alt="Gim Hee Lee" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Gim_Hee_Lee"><a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a></h2>
              <p>National University of Singapore</p>
              <div class="social">
                <a href="https://www.facebook.com/lgimhee"><i class="fa fa-facebook"></i></a>
                <a href="https://www.linkedin.com/in/gim-hee-lee-67764b50/"><i class="fa fa-linkedin"></i></a>
                <!--
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
              </div>
              <p>Bio:<br>Gim Hee Lee is currently an Assistant Professor at the <a href="https://www.comp.nus.edu.sg/about/depts/cs/">Department of Computer Science</a> at the <a href="http://www.nus.edu.sg/">National University of Singapore (NUS)</a>, where he heads the Computer Vision and Robotic Perception (CVRP) Laboratory. He is also affiliated with the <a href="http://ids.nus.edu.sg/">NUS Institute of Data Science</a>. He was a researcher at <a href="http://www.merl.com/">Mitsubishi Electric Research Laboratories (MERL)</a>, USA. Prior to MERL, he did his PhD in <a href="https://inf.ethz.ch/">Computer Science at ETH Zurich</a>, and B.Eng with first class honors and M.Eng degrees in Mechanical Engineering at NUS. His research interests include computer vision, robotic perception and machine learning. He serves as an Area Chair for <a href="https://bmvc2020.github.io/">BMVC 2020</a>, <a href="http://3dv2020.dgcv.nii.ac.jp/">3DV 2020</a> and <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>, and will be part of the organizing committee as the Exhibition/Demo Chair for CVPR 2023.</p>
              <p>Keynote Title:<br>Semi-supervised 3D Object Detection</p>
	          <p>Keynote Abstract:<br>Despite the promising results, many existing deep learning-based 3D object detection algorithms are highly dependent on strongly supervised learning with large amounts of ground truth training data. However, the 3D object bounding box ground truth labels are often laborious and costly to obtain due to the sparsity and amodality of the 3D point clouds. In this talk, I will present two of our recent works on semi-supervised learning to alleviate the need for large amounts of ground truth labels for 3D object detection.  The first work is on cross-category semi-supervised learning, where knowledge is transferred from strong classes with ground truth labels to weak classes without ground truth labels. The second work is on in-category semi-supervised learning, where knowledge is propagated within each class with few ground truth labels.</p>
			  <a href="https://drive.google.com/file/d/1CE46A5l50u47C7PaFIiXZEBuHYCTjjJm/view?usp=sharing">[Talk Video]</a>
            </div>
          </div>
        </div>

        <p></p>

        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Xiaodan_Liang.jpg" alt="Xiaodan Liang" class="img-fluid" style="width:100%">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Xiaodan_Liang"><a href="https://lemondan.github.io/">Xiaodan Liang</a></h2>
              <p>Sun Yat-sen University</p>
              <div class="social">
                <a href="https://twitter.com/XiaodanLiang"><i class="fa fa-twitter"></i></a>
                <a href="https://www.facebook.com/xiaodan.liang.562"><i class="fa fa-facebook"></i></a>
                <!--
                <a href=""><i class="fa fa-google-plus"></i></a>
                <a href=""><i class="fa fa-linkedin"></i></a>
                -->
              </div>
              <p>Bio:<br>Currently, Dr. Xiaodan Liang is an Associate Professor at the School of Intelligent Systems Engineering, Sun Yat-sen University. She is co-supervising the HCP-I2 Lab SYSU. Before that, she was a Project Scientist in Machine Learning Department, Carnegie Mellon University, working with Prof. Eric P. Xing. She obtained her Ph.D. degree in the School of Data and Computer Science at Sun Yat-sen University, advised by Prof. Liang Lin. She was a visiting scholar in the Department of EECS of the National University of Singapore, working with Prof. Shuicheng Yan. She has closely collaborated with Dr. Xiaohui Shen in Adobe Research and Dr. Jianchao Yang in SnapChat Research.</p>
              <p>Keynote Title:<br>Towards Efficient and Transferrable Network Architecture Search for Visual Recognition</p>
              <!-- <p>Keynote Abstract:<br>TBD.</p> -->
            </div>
          </div>
        </div>

        <p></p>

        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Cees_Snoek.jpg" alt="Cees Snoek" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Cees_Snoek"><a href="http://www.ceessnoek.info/">Cees G. M. Snoek</a></h2>
              <p>University of Amsterdam</p>
              <div class="social">
                <a href="https://twitter.com/cgmsnoek"><i class="fa fa-twitter"></i></a>
                <a href="https://www.facebook.com/cees.snoek.1"><i class="fa fa-facebook"></i></a>
                <a href="https://www.linkedin.com/in/cgmsnoek/"><i class="fa fa-linkedin"></i></a>
                <!--
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
              </div>
              <p>Bio:<br>Dr. Cees G.M. Snoek is a full professor in computer science at the University of Amsterdam, where he heads the Intelligent Sensory Information Systems Lab. He is also a director of three public-private AI research labs: QUVA Lab with Qualcomm, Atlas Lab with TomTom and AIM Lab with the Inception Institute of Artificial Intelligence. At University spin-off Kepler Vision Technologies he acts as Chief Scientific Officer. Professor Snoek is also the director of the master program in Artificial Intelligence and co-founder of the Innovation Center for Artificial Intelligence.</p>
			  <p>He received the M.Sc. degree in business information systems (2000) and the Ph.D. degree in computer science (2005) both from the University of Amsterdam, The Netherlands. His research interests focus on video and image recognition. He has published over 200 refereed book chapters, journal and conference papers, and frequently serves as an area chair of the major conferences in computer vision and multimedia.</p>
			  <p>Professor Snoek is the lead researcher of the award-winning MediaMill Semantic Video Search Engine, which is the most consistent top performer in the yearly NIST TRECVID evaluations. Please look at his website for the detailed record of his awards and past experiences.</p>
              <p>Keynote Title:<br>De-biasing Algorithms for Images Seen and Unseen</p> 
	          <p>Keynote Abstract:<br>It is well known that datasets in computer vision have a strong built-in bias as they can represent only a narrow view of the visual world. Even though addressing biases from the start of the dataset creation is highly recommended, models learned from such data can still be affected by spurious correlations and produce unfair decisions. In this talk I present two algorithms that try to mitigate this bias. For seen images we identify a bias direction in the feature space that corresponds to the main direction of maximum variance of class-specific prototypes. In light of this, we propose to learn to map inputs to domain-specific embeddings, where each value of a protected attribute has its own domain. For unseen images, in a generalized zero-shot learning setting, we propose a bias-aware learner to map inputs to a semantic embedding space. During training, the model learns to regress to real-valued class prototypes in the embedding space with temperature scaling, while a margin-based bidirectional entropy term regularizes seen and unseen probabilities. Experiments demonstrate the benefits of the proposed de-biased classifiers in multi-label and zero-label settings, as well as their ability to improve fairness of the predictions.</p> 
			  <a href="https://drive.google.com/file/d/1S30CXzwsxwBjlmgRU5tL3CCzuKdgg-fp/view?usp=sharing">[Talk Video]</a>
            </div>
          </div>
        </div>
	      
        <p></p>
		
		<div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Atlas_Wang.jpg" alt="Zhangyang (Atlas) Wang" class="img-fluid" style="width:100%">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Atlas_Wang"><a href="https://vita-group.github.io/">Zhangyang (Atlas) Wang</a></h2>
              <p>University of Texas at Austin</p>
              <div class="social">
                <!--
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                <a href=""><i class="fa fa-linkedin"></i></a>
                -->
              </div>
              <p>Bio:<br>Zhangyang (Atlas) Wang is an Assistant Professor in the Department of Electrical and Computer Engineering at the University of Texas at Austin. He graduated with a Ph.D. from UIUC, advised by Prof. Thomas S. Huang. Before joining the University of Texas at Austin, he was a Research Scientist in the Department of Industrial and Systems Engineering at the University of Washington. His research interests are mainly in machine learning and computer vision. His recent works focus on the following areas: (a) Enhancing deep learning robustness, efficiency, and privacy/fairness. (b) Deep learning for optimization, and optimization for deep learning. (c) Applications: Computer vision and interdisciplinary problems.</p>
              <p>Keynote Title:<br>Robustness from Unlabeled Data</p> 
              <p>Keynote Abstract:<br>Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We first introduce adversarial training into self-supervision, to provide general-purpose robust pretrained models for the first time. We find these robust pretrained models can benefit the subsequent fine-tuning in two ways: i) boosting final model robustness; ii) saving the computation cost, if proceeding towards adversarial fine-tuning. Further, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. The new approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. Our results exemplify the important role that unlabeled data could play in advancing machine learning robustness.</p> 
			  <a href="https://drive.google.com/file/d/1SYL1kR1Jy7lJq5RKYW0Y0a2LqquBRdh-/view?usp=sharing">[Talk Video]</a>

            </div>
          </div>
        </div>

        <p></p>

        <div class="row">
          <div class="col-md-6">
            <img src="img/speakers/Junsong_Yuan.jpg" alt="Junsong Yuan" class="img-fluid" style="width:100%;padding-bottom: 0px">
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Junsong_Yuan"><a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a></h2>
              <p>University at Buffalo, State University of New York</p>
              <div class="social">
                <!--
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
                <a href="https://www.facebook.com/junsong.yuan"><i class="fa fa-facebook"></i></a>
                <a href="https://www.linkedin.com/in/junsong-yuan-005861b/"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Junsong Yuan is currently an Associate Professor and Director of Visual Computing Lab at the Department of Computer Science and Engineering (CSE), State University of New York at Buffalo, USA. Before that he was an Associate Professor at Nanyang Technological University (NTU), Singapore. He obtained his Ph.D. from Northwestern University, M.Eng. from National University of Singapore and B.Eng from the Special Program for the Gifted Young of Huazhong University of Science and Technology (HUST), China. His research interests include computer vision, pattern recognition, video analytics, gesture and action analysis, large-scale visual search and mining. He received Best Paper Award from IEEE Trans. on Multimedia, Nanyang Assistant Professorship from NTU, and Outstanding EECS Ph.D. Thesis award from Northwestern University. He is currently Senior Area Editor of Journal of Visual Communications and Image Representation (JVCI), Associate Editor of IEEE Trans. on Image Processing (T-IP) and IEEE Trans. on Circuits and Systems for Video Technology (T-CSVT), and served as Guest Editor of International Journal of Computer Vision (IJCV). He is Program Co-Chair of IEEE Conf. on Multimedia Expo (ICME'18) and Steering Committee Member of ICME (2018-2019). He also served as Area Chair for CVPR, ICIP, ICPR, ACCV, ACM MM, WACV etc. He is a Fellow of International Association of Pattern Recognition (IAPR).</p>
              <p>Keynote Title:<br>Visual Learning with Less Labeled Data</p>
			  <a href="https://drive.google.com/file/d/1ik0wtSJs58yst5xOziW5DqFDi3s6-ffz/view?usp=sharing">[Talk Video]</a>
			  <!-- <p>Keynote Abstract:<br>TBD.</p> -->

            </div>
          </div>
        </div>



      </div>

    </section>

  </main>


  <!--==========================
    Footer
  ============================-->
  <footer id="footer">
  <!--
    <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-3 col-md-6 footer-info">
            <img src="img/logo.png" alt="TheEvenet">
            <p>In alias aperiam. Placeat tempore facere. Officiis voluptate ipsam vel eveniet est dolor et totam porro. Perspiciatis ad omnis fugit molestiae recusandae possimus. Aut consectetur id quis. In inventore consequatur ad voluptate cupiditate debitis accusamus repellat cumque.</p>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-contact">
            <h4>Contact Us</h4>
            <p>
              A108 Adam Street <br>
              New York, NY 535022<br>
              United States <br>
              <strong>Phone:</strong> +1 5589 55488 55<br>
              <strong>Email:</strong> info@example.com<br>
            </p>

            <div class="social-links">
              <a href="#" class="twitter"><i class="fa fa-twitter"></i></a>
              <a href="#" class="facebook"><i class="fa fa-facebook"></i></a>
              <a href="#" class="instagram"><i class="fa fa-instagram"></i></a>
              <a href="#" class="google-plus"><i class="fa fa-google-plus"></i></a>
              <a href="#" class="linkedin"><i class="fa fa-linkedin"></i></a>
            </div>

          </div>

        </div>
      </div>
    </div>
	-->
	
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong>TheEvent & VASBSD 2021</strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=TheEvent
        -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- #footer -->

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>
</body>

</html>
